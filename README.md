# 🧠 RAG-Powered Chatbot with FastAPI, Ollama, and Chroma

This is a lightweight **Retrieval-Augmented Generation (RAG)** application built using:

- **FastAPI** – for exposing a simple API
- **ChromaDB** – to store vector embeddings of documents
- **Ollama + LLaMA3** – for running a local LLM
- **LangChain** – to orchestrate LLM and document retrieval

---

## 🚀 Features

- Upload PDFs → Chunk and embed content
- Store embeddings in **ChromaDB**
- Query over your documents via a FastAPI endpoint
- Answers are generated by **LLaMA3 (via Ollama)** using retrieved context

---

## 🧩 Stack

| Component        | Tech                        |
|------------------|-----------------------------|
| Backend API      | FastAPI                     |
| Vector Store     | Chroma                      |
| Embeddings       | HuggingFace Transformers    |
| Language Model   | Ollama (`llama3`)           |
| Orchestration    | LangChain                   |

---

## 📦 Setup Instructions

### 1. 🧱 Clone the Repository

```bash
git clone https://github.com/Manish396/RAG-pdf_text_retrival-llama.git
cd RAG-pdf_text_retrival-llama
```

### 2. 🐍 Create and activate Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. 📦 Install Requirements
```bash
pip install -r requirements.txt
```

### 4. 🧠 Install and Run Ollama
Install Ollama from: https://ollama.com

Then run the server and pull the model:
```bash
ollama serve
ollama run llama3
```

### 5. 📄 Embed Your Documents
Create a script to load and chunk PDF files, then store their embeddings in ChromaDB using HuggingFaceEmbeddings.

Make sure the ChromaDB directory is created before you start the API:
```bash
python docVectorDBEmbedding.py
```

### 6. 🚀 Start the FastAPI Server
```bash
python main.py
# OR use:
uvicorn main:app --reload
```
Access the endpoint:
```bash
GET http://localhost:8000/ask?question=Your+query+here
```

📌 Requirements
Python 3.10+

Ollama (for local LLMs)

Uvicorn / FastAPI / LangChain / Chroma